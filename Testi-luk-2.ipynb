{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tiedettä tekemässä: isojen mittausten äärellä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kuten olet varmaan huomannut, sana \"data\" esiintyy nykymaailmassa vähän joka paikassa, niin koulussa kuin vaikka kotona nettiä selaillessakin. Lauseet kuten \"Google kerää käyttäjädataa\", \"ilmastotieteilijöiden keräämän datan mukaan\" tai \"Heartbleed-datavuoto aiheutti suuren riskin erinäisten palvelujen käyttäjille\" ovat nykyään uutisoinnin arkipäivää.\n",
    "\n",
    "Mutta mitä on data, erityisesti nyttemmin esiin noussut Big Data? Mistä kaikkialta sitä tulee, miksi sitä kerätään ja miten sitä käytetään? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/SPPaTJUYenbLYZTuwIJw7a08nz55YDANDkXE1mqYtveXgkh3PwvEO5o9Wa14ctwiwIIWlhLQFBQeWJpFBOiwATxJJnuRtrnnP4ZckW8wFtSTJDL6EmewLZSbQwkpr6IRmSEZJqoABKYQrQIRtRnriiU5fOoSOUXaMeSIP_SO3fLlzrr2P4FKEGvd_H_aprTFv1xcOKVSIXnLKTdnQB0sRFeJxLjHroRoEpZNz6gXkK-RaWiSi0rbioWMMCdLpaFqHnJyr1H8QWriBIm7a5qI7s36GH5Ddzvic5IFA0mtts-Ln7vc2NyBrs5p6KDNxyHnQtIWl5dtKLg6IgrdqUq5ga80oQhT3rrvLOXss75dfguK46WlQq1Et2vzOoWgiHqVfnHWhOWJ213BF_IXkeEUufPWmXCHuEn3JLxPiDUX5hT772V_J4jm06hKHQ7X6dVhy-LPvTbbOVoJ3GPsyo_s89IRl-fp2wOmylSUgky-p3vbLJiKI2vfwuSlEfPCfs-MXqVPEwnAbwz9RyY5VNfSpiYFQ6QUZAwUC8oI6qUh88EYLFx1RDdiQTdka0XmGcJBfoioQuYQaz3Ec-KERF32BkOGFZsXcHX8MggoBJ4=w671-h546-no\" width=\"700px\">\n",
    "\n",
    "(Kuva muokattu artikkelista https://www.wired.com/2013/04/bigdata/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kun muistetaan terabitin olevan noin 1000 gigabittiä, puhutaan valtavista määristä tietoa. Tätä tietoa kerätään eri muodoissa, sekä tarkoituksella että tarkoituksetta. Luonnontieteellisten kysymysten äärellä asioita mittaillaan useimmiten yritettäessä joko luokitella ympäröivää luontoa (eläinpopulaatioiden koot, peltoalueiden vuosituotannot...) tai etsittäessä vastauksia kiinnostaviin ongelmiin (missä ilmaston lämpeneminen näkyy pahiten, minne ihmiselimistössä kertyy vaarallisia aineita, mitä tapahtuu atomitason hiukkastörmäyksissä ja mitä niistä syntyy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ja sen jako"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suuri osa ihmiskunnan tuottamasta datasta on luonteeltaan salattua, se kun sisältää yritysten liikesalaisuuksia, valtioiden turvallisuusanalyysejä tai muita tekijöitä, joita ei välttämättä haluta näyttää ihan kaikille. Tiedeyhteisön tapauksessa on ideaalitilanteessa toisin: erityisesti luonnontieteissä oltaisiin parhaassa mahdollisessa maailmassa, mikäli tutkimukset voitaisiin jakaa suoraan ympäri maailmaa mahdollisimman monien nähtäville arvioitaviksi ja toistettaviksi, jotta nähtäisiin onko tuloksissa ja päätelmissä puutteita. Tämä ei aina toteudu, kuten vaikkapa lääkeyhtiöiden kohdennetun tutkimuksen jäädessä salaisuudeksi. Suljetuissa julkaisuissa hukataan kiinnostavia sivupolkuja, jotka laaja perustutkimus olisi voinut avata.\n",
    "\n",
    "Avoimesti saatavilla olevaa dataa on kuitenkin paljon ja tässä harjoitteessa lähdetään tutustumaan hieman siihen, miten sitä voisi hyödyntää erityisesti fysiikan yhteydessä. Harjoitteessa käytetään vähän ohjelmointia, kuten oikeassakin tieteellisessä laskennassa, mutta ei mitään hätää jos Python ei ole aiemmin tuttua. Tässähän sitä oppii!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CERN, LHC ja hiukkastutkimus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alustetaan ensin vähän tietokonettamme valikoimalla sille käyttöön muutama Pythonin tarvitsema paketti. Huomaa, että kukin koodinpätkä on tässä Notebookissa omana solunaan, jonka saat ajettua valitsemalla sen hiirellä ja painamalla Ctrl+Enter. Kun vieressä olevassa In [ ] -kohdassa näkyy * , koodia ajetaan. Jos siinä näkyy numero, solu on ajettu.\n",
    "\n",
    "Aja soluja mieluiten ensin järjestyksessä, jotta tietokone ymmärtää muuttujat oikein. Sen jälkeen voit alkaa vapaasti leikkimään ja kokeilemaan mitä koodilla saakaan aikaan. Jos kuitenkin saat notebookin jotenkin solmuun, paina yläriviltä \"Kernel\"-valikosta Restart & Clear Output, joka pyyhkii muutokset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Olen kommentti! Risuaitamerkeillä ja kommenteilla koodin saa paljon helppolukuisemmaksi.\n",
    "\n",
    "# Haetaan tarvittavat paketit. Pandas on data-analyysiä varten, numpy tieteellistä laskentaa ja\n",
    "# matplotlib.pyplot mahdollistaa kuvaajien piirtämisen. Annetaan näille lyhyemmät nimet (pd, np ja plt),\n",
    "# jotta myöhemmin niitä käytettäessä ei tarvitse kirjoittaa koko paketin nimeä.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CERN:in suurella koelaitteistolla, Large Hadron Colliderilla, päästään tutkimaan aineen perusluonnetta ja alkeishiukkasia paremmin kuin missään muualla maapallolla. Tutkijat keräävät miljardeja mittaustuloksia hiukkastörmäyksistä ja analysoivat näitä selvittääkseen, mitä niissä oikein tapahtuikaan.\n",
    "\n",
    "[Kurkista törmäykseen tällä 3D-mallilla!](http://opendata.web.cern.ch/visualise/events/cms#)\n",
    "\n",
    "Yksi koeasema LHC:n yhteydessä, CMS (Compact Muon Solenoid), julkaisee suuria määriä dataa avoimeen jakoon ([kuten nämä](http://opendata.web.cern.ch/record/545)). Tutkitaan tässä yhtä sellaista settiä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 20 fields in line 5, saw 302\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b031ab958a56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Luodaan uusi DataFrame-rakenne CMS:n mittausdataa sisältävästä tiedostosta \"Zmumu_Run2011A_massoilla.csv\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Annetaan luodulle DataFramelle nimi 'datasetti'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdatasetti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://drive.google.com/open?id=1zq0GLuedfjFNonMaQdjNtZdAPUW6KIMf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1005\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1748\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1749\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11138)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas\\_libs\\parsers.c:11884)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows (pandas\\_libs\\parsers.c:11755)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error (pandas\\_libs\\parsers.c:28765)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 20 fields in line 5, saw 302\n"
     ]
    }
   ],
   "source": [
    "# Luodaan uusi DataFrame-rakenne CMS:n mittausdataa sisältävästä tiedostosta \"Zmumu_Run2011A_massoilla.csv\".\n",
    "# Annetaan luodulle DataFramelle nimi 'datasetti'.\n",
    "datasetti = pd.read_csv('https://drive.google.com/open?id=1zq0GLuedfjFNonMaQdjNtZdAPUW6KIMf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
